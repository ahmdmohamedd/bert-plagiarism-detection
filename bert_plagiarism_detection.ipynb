{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5352b168-fd53-428a-b2a1-2f82812b36d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c3abc5a-3ff5-4719-87a3-a11b32417c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample document used testing plagiarism detection'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Example preprocessing\n",
    "example_text = \"This is a sample document used for testing plagiarism detection.\"\n",
    "preprocessed_text = preprocess_text(example_text)\n",
    "preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb192582-b3e6-404c-b460-6bf5a10ce2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>This is a sample document used for testing pla...</td>\n",
       "      <td>[This is a sample document used for testing pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This is a test document designed to evaluate p...</td>\n",
       "      <td>[This is a test document designed to evaluate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>She opened the book and immediately got lost i...</td>\n",
       "      <td>[She opened the book and immediately got lost ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                               text  \\\n",
       "0       1  This is a sample document used for testing pla...   \n",
       "1       2  This is a test document designed to evaluate p...   \n",
       "2       3  She opened the book and immediately got lost i...   \n",
       "\n",
       "                                           sentences  \n",
       "0  [This is a sample document used for testing pl...  \n",
       "1  [This is a test document designed to evaluate ...  \n",
       "2  [She opened the book and immediately got lost ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset and split into sentences\n",
    "data = {\n",
    "    'doc_id': [1, 2, 3],\n",
    "    'text': [\n",
    "        \"This is a sample document used for testing plagiarism detection.\",\n",
    "        \"This is a test document designed to evaluate plagiarism detection\",\n",
    "        \"She opened the book and immediately got lost in the story's magical world.\"\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Tokenize documents into sentences\n",
    "df['sentences'] = df['text'].apply(sent_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e762a7b9-f68b-4828-9178-fa46e50106ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings for a sentence\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach()\n",
    "\n",
    "# Example usage\n",
    "sample_sentence = \"Plagiarism detection using BERT model.\"\n",
    "embedding = get_bert_embedding(sample_sentence)\n",
    "embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96aa17c9-0585-4da3-a5d1-1f0f01c5fcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare document sentences and calculate similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(doc_sentences, source_sentences):\n",
    "    plagiarism_score = 0\n",
    "    total_sentences = len(doc_sentences)\n",
    "    \n",
    "    for doc_sentence in doc_sentences:\n",
    "        doc_embedding = get_bert_embedding(doc_sentence)\n",
    "        max_similarity = 0\n",
    "        \n",
    "        for source_sentence in source_sentences:\n",
    "            source_embedding = get_bert_embedding(source_sentence)\n",
    "            similarity = cosine_similarity(doc_embedding, source_embedding)[0][0]\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "        \n",
    "        if max_similarity > 0.8:  # Similarity threshold for plagiarism\n",
    "            plagiarism_score += 1\n",
    "    \n",
    "    return plagiarism_score / total_sentences\n",
    "\n",
    "# Example comparison between a document and a source\n",
    "doc_sentences = sent_tokenize(\"Plagiarism detection using advanced techniques in NLP.\")\n",
    "source_sentences = sent_tokenize(df['text'][1])  # Source document\n",
    "\n",
    "similarity_score = calculate_similarity(doc_sentences, source_sentences)\n",
    "similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8981b528-dea7-4804-8111-e6ed099b4566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1.0), (2, 1.0), (3, 0.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plagiarism detection function\n",
    "def detect_plagiarism(doc_text, sources_df):\n",
    "    doc_sentences = sent_tokenize(doc_text)\n",
    "    plagiarism_scores = []\n",
    "    \n",
    "    for idx, row in sources_df.iterrows():\n",
    "        source_sentences = row['sentences']\n",
    "        similarity_score = calculate_similarity(doc_sentences, source_sentences)\n",
    "        plagiarism_scores.append((row['doc_id'], similarity_score))\n",
    "    \n",
    "    return plagiarism_scores\n",
    "\n",
    "# Example detection\n",
    "plagiarized_scores = detect_plagiarism(\"This is a sample document used for testing plagiarism detection.\", df)\n",
    "plagiarized_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6076b41-a08c-47c7-9d89-989853d87ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plagiarism Results:\n",
      "Source Document 1: 100.00% of content matches.\n",
      "Source Document 2: 100.00% of content matches.\n",
      "Source Document 3: 0.00% of content matches.\n"
     ]
    }
   ],
   "source": [
    "# Output the plagiarism results\n",
    "def report_plagiarism_results(plagiarized_scores):\n",
    "    print(\"Plagiarism Results:\")\n",
    "    for doc_id, score in plagiarized_scores:\n",
    "        print(f\"Source Document {doc_id}: {score*100:.2f}% of content matches.\")\n",
    "\n",
    "# Example report\n",
    "report_plagiarism_results(plagiarized_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
